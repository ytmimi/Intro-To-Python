{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap Last Week\n",
    "\n",
    "We covered:\n",
    "* Reading and Writing csv files with the built in CSV module\n",
    "* installing 3rd party packages with pip\n",
    "* Reading and Writing Excel files with the openpyxl module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Week\n",
    "\n",
    "what we will cover:\n",
    "\n",
    "* Pandas basic data structures: Series and Dataframes\n",
    "* Basics of Data analysis with Pandas\n",
    "* Data visualization with matplotlib\n",
    "\n",
    "This week we'll be using pandas to analyse data, and matplot lib to visualize the data. If you don't have these libraries installed, open up a terminal window or command prompt and type:\n",
    "\n",
    "    pip install pandas matplotlib\n",
    "\n",
    "This will install all the libraries we'll need for this class.\n",
    "\n",
    "We'll also be analyzing a UFO sighting dataset that you find can find [here](https://www.kaggle.com/NUFORC/ufo-sightings). Once you make a free account you'll have access to that, and many more data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Pandas?\n",
    "\n",
    "Pandas, short for **panel data**, is a 3rd party module in Python, which means you'll need to install it before you can use it.\n",
    "    \n",
    "Pandas is a library that provides high level data structures and manipulation tools to make data analysis fast, easy, and enjoyable in Python. Pandas is built on top of NumPy -- an extreamly efficient scientific computing library for Python -- and if you want to learn more about the engine that powers pandas I highly encourage you to check out these links:\n",
    "\n",
    "* [Numpy Basics](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html)\n",
    "* [Numpy Array Computation](https://jakevdp.github.io/PythonDataScienceHandbook/02.03-computation-on-arrays-ufuncs.html)\n",
    "* [Numpy Aggregation Functions](https://jakevdp.github.io/PythonDataScienceHandbook/02.04-computation-on-arrays-aggregates.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with Pandas\n",
    "\n",
    "In general when working with pandas, you'll rely on two main data structures: ``Series`` and ``DataFrame``\n",
    "\n",
    "Typicall import convention is: ``import pandas as pd``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Series\n",
    "\n",
    "A Series is a one-dimensional, list-like object provided by pandas. The data will be stored in a Numpy array, and will also have an associated array of labels, otherwise known as the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Series from a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(['Monday', 'Tuesday', 'Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "\n",
    "print(type(ser1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ser1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, when you print out the string representation of a Series object, you'll see the list of data on the right hand side and the list of labels on the left hand side. Because we didn't supply an index a numeric index was provided for us. This is the default behavior when working with both Series and Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can directly access the array data of our Series object using the .values attribute, and the labels of our Series using the .index attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Series with an index\n",
    "\n",
    "It's usually useful to include an index for the data that you're trying to analyze. This makes it much easier to locate specific values within the Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2 = pd.Series([1, 2, 3, -6, 7, 0, 5, -4], index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n",
    "print(ser2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you'll notice this time the index is set to exactly the values that we specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing values from the Series\n",
    "\n",
    "You can use both the array index, or row labels to access values stored in a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access via the array index\n",
    "print(ser2[0])\n",
    "\n",
    "# access via the index label\n",
    "print(ser2['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Multiple values from the Series\n",
    "\n",
    "If you need to access multiple values, you can pass a list of indexs or labels. This type of value access is made possible becuase of Numpy, and I highly encourage you to go back and check out the links I provided earlier if you want to learn more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ser2[[0, 1, 2]])\n",
    "\n",
    "\n",
    "print(ser2[['d', 'e', 'f']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassigning Values in the Series\n",
    "\n",
    "It's also fairly easy to reassign values of any Series. In order to not change the orginal Series, we'll make a copy of it using the ``.copy`` method first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser3 = ser2.copy()\n",
    "print(ser3, end='\\n\\n')\n",
    "\n",
    "ser3['d'] = 100\n",
    "print(ser3, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that reassigning values in a series is syntactically identical to reassigning values in a Python dictionary. In a way, a Series is a super charged dictionary because it allows you to access values via their row labels (keys), and via their index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Series from a Dictionary\n",
    "\n",
    "In fact, you can pass a dictionary as an argument when instantiating a Series, and the keys will automatically be assigned as the row labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nobel gasses from the period table: mapping names to atomic numbers\n",
    "# data found at: https://en.wikipedia.org/wiki/Noble_gas\n",
    "noble_gases  = {'Helium':2, 'Neon':10, 'Argon':18, 'Krypton':36, 'Xenon':54,'Radon':86}\n",
    "\n",
    "ng_series = pd.Series(noble_gases)\n",
    "# string representation of a the ng_series\n",
    "print(ng_series, end='\\n\\n')\n",
    "\n",
    "# indexing the first value of the series\n",
    "print(ng_series[0], end='\\n\\n')\n",
    "\n",
    "# using dictionary lookup to find the value of 'Argon'\n",
    "print(ng_series['Argon'], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the Series\n",
    "\n",
    "You can create Boolean indexes by using logical operations on a Series. A boolean index is just a Series where each data point is either True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a boolean index for all values less than 36\n",
    "ng_series < 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use these Boolean indexes to filter the data in the Series. Only values that evaluate to True in the Boolean index will remain when filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the original Series where the atomic number is less than 36\n",
    "ng_series[ng_series < 36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrame\n",
    "\n",
    "A DataFrame is a tabular data structure provided by Pandas. You can think of a DataFrame as a collection of Series with the same index. A DataFrame is probably the data structure that you'll use the most in pandas, and it is indexed by both its row labels and its column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataframe from a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(matrix)\n",
    "\n",
    "# What's the Type of a DataFrame?\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The string representation of a DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar to a Series, if you don't provide a specific index, a numeric index will be assigned automatically. In the previous example we didn't specify a column or row index so pandas provided one for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the index using ``.index`` attribute, and you can use the ``.columns`` attribute to list the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassigning the index (Column and Row)\n",
    "We can reassign the index if we need to by providing a list of of values with a lenght equal to the length of the current index. Both the column headers and the row labels are 3 items long, so in order to replace them we can assign a list of 3 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = ['a', 'b', 'c']\n",
    "df.columns = ['col1', 'col2', 'col3']\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a DataFrame with indexes already assigned\n",
    "\n",
    "It's often easier to assign column and row labels when instantiating the DataFrame object. You can do so by providing index and columns keyword arguments when instantiating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[90, 87, 85, 76], [93, 91, 91, 84], [70, 75, 80, 99], [83, 88, 91, 77]]\n",
    "students = ['Bob', 'Alice', 'Carter', 'Dan']\n",
    "subject = ['Math', 'Science', 'History', 'English']\n",
    "\n",
    "test_scores = pd.DataFrame(scores, index=subject, columns=students)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing DataFrame Columns\n",
    "\n",
    "Accessing Values from a Series was fairly straightforward because there was only a single axes where data was stored. Accessing data from a DataFrame is slightly different becuase you can access Data via the row or the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Dictionary look up only works for the Columns of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Series of Scores for Bob\n",
    "bob_col = test_scores[\"Bob\"]\n",
    "\n",
    "print(bob_col, end='\\n\\n')\n",
    "\n",
    "print(type(bob_col), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: As previously mentioned, each column is itself a Series, with the same index as the entire DataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a list of columns to filter the DataFrame for only the columns we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_and_b = test_scores[[\"Alice\", \"Bob\"]]\n",
    "\n",
    "print(a_and_b, end='\\n\\n')\n",
    "\n",
    "print(type(a_and_b), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Filtering the DataFrame for more than one column will return a DataFrame, NOT A SERIES** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing DataFrame Rows\n",
    "\n",
    "Earlier, we were able to access the values of a Series by either their row label or array index. When working with DataFrames, you can use dictionary lookup to  the Because Dictionary lookup on a DataFrame is reserved for the columns, using slicing notation will result in looking up row. A DataFrame also has two attirbutes which support lookup on the DataFrame rows\n",
    "\n",
    "* loc - used for row look up using the row label\n",
    "* iloc - used for row look up using the index of the row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: trying to index a single row using slicing notation will throw an error!!!!! This is becuase dictionary lookup is assumed, and you'll receive a key error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this wont work\n",
    "# test_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using slicing notation\n",
    "print(test_scores[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_row = test_scores.loc[\"Math\"]\n",
    "print(math_row)\n",
    "print(type(math_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Maybe this is intuitive based on the fact that the column headers are also treated like an index, but taking a row of the DataFrame, provides a Series indexed by the column header of the original DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use slicing sintax with the row labels\n",
    "test_scores.loc[\"Math\":\"History\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can pass a list of row labels\n",
    "test_scores.loc[[\"Science\", \"English\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores.loc[\"Math\", \"Bob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using iloc\n",
    "\n",
    "Again, iloc is used to referect the rows of that DataFrame by their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing a single row\n",
    "test_scores.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Using iloc allows you to access a single row, from the DataFrame, where trying to us index notation will raise a KeyError**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding New Columns to a DataFrame\n",
    "\n",
    "We can assign new columns to a DataFrame by adding a new Series with identical row labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores['Ed'] = pd.Series([72, 80, 79, 77], index=subject)\n",
    "\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a column for Ed has now been added to the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the DataFrame With Boolean Indexing\n",
    "\n",
    "As we did before, we can use Boolean indexing to to filter the DataFrame. If we use a conditonal logic expression on a DataFrame, each cell will be evaluated against the expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores > 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying the boolean DataFrame to the original DataFrame, only True values remain, and the rest are filled with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores[test_scores > 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often more meaningful to filter along a row or a column, for example Filtering For Students with Math Scores better than 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .transpose to flip the column and row labels, because normally we can't use dictionary lookup on row labels\n",
    "test_scores.transpose()[test_scores.transpose()[\"Math\"] > 85]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the new DataFrame only contains values where Math was > 85\n",
    "\n",
    "Look here for more info on [.transpose](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Functions\n",
    "\n",
    "Aggregat function let you quickly and easily gather information on the data that you're working with. The result of an aggregation on a DataFrame is a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the average test score for each student?\n",
    "test_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the highest test score for each student?\n",
    "test_scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the lowest test score for each student?\n",
    "test_scores.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defulat these aggregations will opperate on the columns. If you want to aggregate across the rows, you'll either need to transpose your data (flip the column and rows), or you'll need to change the axis that the aggregation is applied to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the average test score in each subject?\n",
    "test_scores.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the highest test score in each subject?\n",
    "test_scores.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the lowest test score in each subject?\n",
    "test_scores.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what was the standard deviation of test scores in each subject?\n",
    "test_scores.std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bit about efficiency\n",
    "\n",
    "As you've become more familiar with Python, you've gotten comfortable with the **for loop**. When working with pandas, it's generally not a good idea to use for loops. because they are **EXTREMELY** inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random_df = pd.DataFrame(np.random.randn(100, 100))\n",
    "\n",
    "print(random_df.head())\n",
    "\n",
    "def square(value):\n",
    "    return value ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a normal for Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for row in random_df:\n",
    "    square(random_df.iloc[row])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using iterrows (if you absolutely have to loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for index, row in random_df.iterrows():\n",
    "    square(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use .apply, it's more efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "random_df.apply(square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "random_df.apply(np.square)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "random_df ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "np.square(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas code generally runs more efficently when it's vectorized. As you can see from above the two solutions that were the most efficient were the ones that used the optimized numpy functions. Also note that using .iterrows and .apply were faster than using the for loop. These were all numeric calculations so they ran pretty quickly anyway, but you could forseeably have other data types, and other opperations that you'd like to apply to an entire DataFrame or Series, and you'll almost always see better performace if you:\n",
    "\n",
    "    1) user .apply and .iterrows\n",
    "    2) use optimized numpy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "We'll take what we've learned about how to work with pandas DataFrames and Series, and apply that to analyzing a [ufo sighting](https://www.kaggle.com/NUFORC/ufo-sightings) dataset. If you haven't already done so, please download and unzip the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# You technically don't need a Path object here, but I like using it because it alllows me to create file system paths\n",
    "# that work regardless of the operating system\n",
    "ufo_csv_path = Path('ufo-sightings/scrubbed.csv')\n",
    "ufo_csv_path.absolute()\n",
    "\n",
    "ufo_df = pd.read_csv(ufo_csv_path)\n",
    "# Pandas has the ability to quickly read data in from may file formats for example:\n",
    "# Excel, Json, HTML, SQL, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the Data\n",
    "\n",
    "When doing any exploratory data analysis it's usually a good idea to check out our data and see what we're working with. You can use the ``.head`` meathod of a pandas Series or DataFrame to inspect the first few rows. You can also use the ``.tail`` method to inspec the last few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames also have a useful ``.describe`` method, which makes it really easy to get some quick statistics on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: the .describe method will only evaluate numeric data types.**\n",
    "\n",
    "The main take away from looking at the result of ``.describe`` is realizing that there are **over 80,000 rows** in our dataset and pandas is able to handle that easily!!!\n",
    "\n",
    "Because longitude was the only column that got evaluated, I'm thinking its possible that not all the data is of the approprate datatype. This might be because we read the data in from a csv file. We can quicly check that datatype of each column by using the ``.dtypes`` attribute of a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see **object**, that's pandas way of saying that the values are strings. You'll also note that longitude is a **float64**. The important thing to take away here is that Pandas (but really Numpy) is built on top of C --not Python -- which means it stores values as C data types. Being built on C is one of the reasons Pandas is quit fast when written well. Although not all of our data is numeric, lets work on casting as much as we can into an appropriate datatype. This means:\n",
    "\n",
    "* Casting the 'datetime' column to a datetime type\n",
    "* Casting the 'duration (seconds)' column to an integer\n",
    "* Casting the 'date posted' column to a date\n",
    "* Casting the 'latitude' column to a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There a tons of ways to cast data. In fact you can also cast data types when reading in the data\n",
    "# you can apply a mapping function to a Series \n",
    "ufo_df[['duration (seconds)', 'latitude']] = ufo_df[['duration (seconds)', 'latitude']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "ufo_df['datetime'] = pd.to_datetime(ufo_df['datetime'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "\n",
    "ufo_df['date posted'] = pd.to_datetime(ufo_df['date posted'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# check out the data types again\n",
    "print(ufo_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info on **pd.to_numeric** look [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html)\n",
    "\n",
    "For more info on **pd.to_datetime** look [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Droping NaN\n",
    "\n",
    "Just from inspecting the head and tail of the data we see that there are some NaN values. Normally we'd want to go in and preserve as much data as we could, but for this analysis I'll choose to remove any row containing a NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will remove all rows with NaN in place. \n",
    "ufo_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a better sense of what countries these sightings occured. To get a list of all the unique cities where ufo's were sighted, we can use the .unique method on a pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_df['country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that 4 countries contributed to this dataset. Maybe we'd like to know how many sightings took place in each of these countries. A quick way to do this is to use the .value_counts attribute of a pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = ufo_df['country'].value_counts()\n",
    "print(country_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With very little effort at all we can see that a majority of the data came from the us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy plots (must have matplotlib installed)\n",
    "\n",
    "both Series and DataFrames can be easily graphed using the ``.plot`` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the Data\n",
    "\n",
    "Becaus the us makes up an overwhelming amount of the dataset, lets focus our exploration just on the us sightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ufos = ufo_df[ufo_df['country'] == 'us']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see which cities had the most activity in our dataset, instead of directly accessing the \"state\" column, we'll use a groupby opperation to acheive an identical result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts = us_ufos.groupby('state').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the head of our data, we can see that each column was grouped by state, and the count was taken. You'll also notice that **state** has become the new index of the resulting grouped DataFrame. You should also note that each column now has identical values, which means we can really use any of these columns (or Series), when trying to plot out ufo sightings by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts.sort_values('city', ascending=True)['city'].plot(kind='barh', figsize=(12, 12), title='UFO Sightings By US State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On average, how long are UFO sightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sighting_time = us_ufos['duration (seconds)'].mean() // 60\n",
    "\n",
    "print(f'UFO Sightings are about {avg_sighting_time} minutes long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How long are UFO sightings by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(us_ufos.groupby('state').mean()['duration (seconds)'] // 60).sort_values().plot(kind='barh', figsize=(12, 12), title='UFO Sighting Duration by State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When did UFO Sightings occur?\n",
    "\n",
    "Because we have date and time data spanning several years, we could extract a lot of additional insights into when these sightings occured. For now we'll keep our analysis to sightings per year, and sightings per time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = us_ufos[['datetime', 'date posted']].copy()\n",
    "time_df['year'] = time_df['date posted'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df['hour'] = time_df['datetime'].apply(lambda x: x.hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.groupby('year').count().sort_values('year')['hour'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.groupby('hour').count()['year'].plot(kind='pie', subplots=True, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "* [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) -- Pandas Documentation getting started page\n",
    "* [Pandas Beginner Tutorial](https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/) on learndatasci.com\n",
    "* [Quentin Caudron - Introduction to data analytics with pandas](https://www.youtube.com/watch?v=F7sCL61Zqss&t=2524s), PyData 2017 Conference Workshop. In the 2 hour workshop, Quentin walks you through reading, cleaning, transforming and visualizing your data with pandas\n",
    "* [Intro to Pandas Tutorial Series](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfSfqQuee6K8opKtZsh7sA9) by sentdex on YouTube\n",
    "* [Speed Up Your Pandas Project](https://realpython.com/fast-flexible-pandas/) -- Blog post about writing more efficient Pandas Code\n",
    "* [Python for Data Analysis Book](http://shop.oreilly.com/product/0636920023784.do), by Wes McKinneey, the creator of Pandas\n",
    "* [High Performance Data Processing in Python](https://www.youtube.com/watch?v=NoJr08FNQeg), PyData 2018 talk by Donald Whyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
