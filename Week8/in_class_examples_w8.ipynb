{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap Last Week\n",
    "\n",
    "We covered:\n",
    "* First class Functions\n",
    "* Functions within Functions\n",
    "* Decorators\n",
    "* Context Managers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Week\n",
    "\n",
    "Interacting with the web:\n",
    "\n",
    "* A Primer on HTTP\n",
    "* The requests module in Python\n",
    "* Getting data from the web\n",
    "* Basics of web scraping with Beautiful Soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of HTTP\n",
    "\n",
    "HTTP is foundational to any exchange of data on the web. HTTP is a **client-server protocol**, which means that clients (often a web browser) must initiate the interaction. Once the server has received a message from the client it can choose whether or not to respond, and what information to return. The messages a client sends are typically called **requests**, and there are several differnt types of requests that a client can make. Whatever the server sends back is know as the **response**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Request Methods\n",
    "\n",
    "Again, When deailing with web interactions the client **IS ALWAYS** the side that makes the **request**. At a minimum each request will contain a **method**, informing the server what action thE client wants to perform, and a **url**, which specifies the path to the resource. HTTP requests typically also contain **headers**, providing the server with additional information about the request, and some HTTP methods will contain a **body** because they are sending data to the server\n",
    "\n",
    "**A list of common HTTP methods**\n",
    "\n",
    "* **GET** - Ask for some resource from the server. Some HTML document, CSS stylesheet, JavaScript file, JSON Data, etc.\n",
    "\n",
    "* **POST** - Used to send data to the server. Typically Form data on an HTML page. Creates new resources on the server\n",
    "\n",
    "* **PUT** - Also used to send data to the server, but is often used to update existing resources instead of creating new ones\n",
    "\n",
    "* **DELETE** - As the name suggests, this method is used to delete resources from the server\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make an http get request, we can use the get method from the requests module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.allrecipes.com/recipes/88/bbq-grilling\"\n",
    "\n",
    "resp = requests.get(url)\n",
    "# The response object has a reference to the request that we just sent\n",
    "req = resp.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_body_position', '_cookies', '_encode_files', '_encode_params', '_get_idna_encoded_host', 'body', 'copy', 'deregister_hook', 'headers', 'hooks', 'method', 'path_url', 'prepare', 'prepare_auth', 'prepare_body', 'prepare_content_length', 'prepare_cookies', 'prepare_headers', 'prepare_hooks', 'prepare_method', 'prepare_url', 'register_hook', 'url']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the request object, and what we can call on it\n",
    "print(dir(req))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the request object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request method was GET\n",
      "\n",
      "The request url was https://www.allrecipes.com/recipes/88/bbq-grilling/\n",
      "\n",
      "The request headers were {'User-Agent': 'python-requests/2.21.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': 'FirstImpression=False; ARSiteUser=1-b7535850-7b50-42b9-99da-094f5638c2ff; ARCompressedSession=LwcAAB+LCAAAAAAABAB9VdmSokoQ/RdfezpYlK3fBEFRAWVTfGMppNgKilUn5t8vbe9zb9yICiLqZFbmycxTxe+ZBa+l33YYzF5mt/0gX+KqgNDny4aEg9+VB5FPbYsySETtV6Ivk5wgKe69Ns5Fwu3ZJy/lb5SgRmNzFhZZfaFrqcuV2o6DiHPDi2yr+kVjIambwEsMxD+BRe9unVGkg2pd02V3XO/EbUHMaQJyhZdhcWE1Tl7vMFemfQtMlTk1JM62TB0RSgJoFfietutXlSNxdy0+3UNq4eW27k1MQ79QXC41stMZBH4+H+9QPaojxWb93Xc8eKvDLBzscLyt5gbVWYWTLFvzHqsj6p4Gh9WNdj2cLzfLoipOdNaD5kgHwKtRHijNgU0F8f5kle7ck6mEK3ahu4szWbnnPVWhSO0U9bi3dgW3XVfzGrlunFFgYXflyJ97KmVDVLhbvA9dc53RCklfxHRhrPZpS8WpoTZ9KA116ZWDMNrMij0QwXBlYOYwB5SKDLskyKkPN04q5k+uoSUtQxtdIWt9UvfnxQK0Cb5E4QbV2YlnJOCLG7Xk0f6qCO2g9KfDoJdLkkmiseKK+yDRF2Nz5mJDrPPFxei8Xsnkc4r2iW5KnZtn1Xp3Ph+NM1fdIU2BHecZ1r4IrRvYqnOS8NGlypP0YnPSsdapqS/79XGTrFLPzMRCD3YaIV8ZEyoN6Ig+3Lo3xy5NciH2Ayv7l7Zmr4pK3lecXtkFOlKhdB031Fzc+7oxT8zTccswSscd4TZ1BktZyR6vkeMypIzsCTkmawoMyxuVprrmxgIqgs14IJhSKr0O+EjmU/pJtNle39siBNEp8bBcnLSzTBghkFY0IW80jXVxzIegDoLSw8vh3hGuKuVou1AW4aG69YlFdzvH8/HNzpSnzDrk7a1uDNsJ8K5LCWGHg+wqrPQ8kA0puETIo8N1eqTIIkFkYhx1bbenJJUnBPLeKTuS6FPEDn3ddPF18Jlje1XFSsHl7sJ646gIqs1H7tnqxD4p1+2qJYZQMDbFChO42y8VzZz9mkkIZRCoZYxmL7/fd+sORtPtpZ4DjpkzPEM+c8H0WdCB8CwIkf9MCouYYed8SMfxV4zV7IX8NVObZdcmoGxh6LdgihP7eQNe8T26XkGklt+gyRVNMPyOmaAARQCwBj5BpwH4PfzD275V4LHb+E2iTjnKLs//vB5WIG5aGxbAhQ1sEZ69tLgDnxa1qDBoGojKL4MJQlgBEY0airocLHsf5n6Qf2XXAYiaAwY9BIMJ4ilA8tNmdVWFcAvLq/ZgbrXTA9j87foo4qPPXdOi4rOoFWyq3L/pfgHeavk1k4uJxjKKXun+wBSEC799HNPB0OSgbQEWYWt1xQM8YBTDHKiFf/0M9jfBJoHVg4ZfhiDPQbSaRvXh/MbNwbmNMlB+5i7fnWYkSVLPj2WT5MtjzR6TEHN0/TZHeawg/iEAF2AYT7fn35Zl2MIefAM+mX1iU1Nx+z8U/jw42H7TgmlWMcBgitD8l4S0qHrdvWrno7qJQYnKW4G65ocw0LWE928kvtr3NuPZCzXlNYoSvv7x3jX3Icj/MCyD4FVHbxdjNvvzD3HpAM0vBwAA; ARToken=84XAr4Z2Z57+ieH6qwuIrbHIXnJ2OTDkWzFfmp3pJ4nmhmSb++hdkKZEZQDxb3y++VlYzQOtyfvkpWburBh7W5hHuxY2qpqVUy54b/fvMXlDi3je2lkg0Ys9x1I/9xpaU43x4HRoZE5kfDcFCKupujnPD5RhqeqzQXnL8yv6g4JMScRNnGGYWw=='}\n",
      "\n",
      "The request body was None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'The request method was {req.method}', end='\\n\\n')\n",
    "print(f'The request url was {req.url}', end='\\n\\n')\n",
    "print(f'The request headers were {req.headers}', end='\\n\\n')\n",
    "print(f'The request body was {req.body}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Responses\n",
    "\n",
    "An HTTP response is what the server sends back after it receives a request. There are a few things that make up a response, The **status code**, **status message**, and **headers**. Optionally, some responses will contain a **body**. Status codes in the **200+** range usually indicate that everything went fine. **300+** requests usually mean there's a redirect, **400+** request mean there's an error with the clients request, and **500+** errors mean that there is an issue with the server.\n",
    "\n",
    "**Common Status Codes**\n",
    "\n",
    "* **200** - The request was succesfull\n",
    "* **301** - What you're request no longer exists at the given url\n",
    "* **400** - Bad Request. The server couldn't figure out what you wanted\n",
    "* **401** - The client needs to authenticate before accessing that resource\n",
    "* **404** - Whatever the client tried to access doesn't exist\n",
    "* **500** - Some internal server error occured\n",
    "\n",
    "**Common Headers**\n",
    "\n",
    "* **Content-Type** - Indicates the media type of the response. Ex) text\\html\n",
    "* **Content-Length** - Indicates how long the body is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the response object\n",
    "\n",
    "Before we made a get request to https://www.allrecipes.com/recipes/88/bbq-grilling, lets inspect the response more thouroughtly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_content', '_content_consumed', '_next', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'is_permanent_redirect', 'is_redirect', 'iter_content', 'iter_lines', 'json', 'links', 'next', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url']\n"
     ]
    }
   ],
   "source": [
    "print(dir(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Status Code of the reponse is 200\n",
      "\n",
      "The response headers are ['Cache-Control', 'Transfer-Encoding', 'Content-Type', 'Content-Encoding', 'Vary', 'Set-Cookie', 'CorrelationId', 'Arr-Disable-Session-Affinity', 'Access-Control-Allow-Origin', 'Date', 'X-F5-Node']\n",
      "\n",
      "The Content-Type is text/html; charset=utf-8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'The Status Code of the reponse is {resp.status_code}', end='\\n\\n')\n",
    "print(f'The response headers are {list(resp.headers.keys())}', end='\\n\\n')\n",
    "print(f\"The Content-Type is {resp.headers['Content-Type']}\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en-us\">\n",
      "<head>\n",
      "    <title>BBQ & Grilling Recipes - Allrecipes.com</title>\n",
      "\n",
      "<script src='https://secureimages.allrecipes.com/assets/deployables/v-1.167.0.5111/karma.bundled.js' async=true></script>\n",
      "\n",
      "\n",
      "    <!--Make our website baseUrl available to the client-side code-->\n",
      "    <script type=\"text/javascript\">\n",
      "        var AR = AR || {};\n",
      "\n",
      "        AR.segmentWriteKey = \"RnmsxUrjIjM7W62olfjKgJrcsVlxe68V\";\n",
      "        AR.baseWebsiteUrl = 'https://www.allrecipes.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The text object contains the html from the response, There's a lot so we're only going to print the first 100 characters\n",
    "print(resp.text[:500], end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we managed to get the actual HTML from the we page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup Basics\n",
    "\n",
    "Beautiful Soup is a 3rd party package in Python, which means that you'll need to install it before you can use it. From a terminal or command prompt type:\n",
    "\n",
    "    pip install beautifulsoup4\n",
    "\n",
    "Beautifyl soup is a library built to easily parse data from html, and it's what we'll use extract data from the request we made earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# pass the html document to Beautiful soup so you can easily parse it\n",
    "soup = BeautifulSoup(resp.text)\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASCII_SPACES', 'DEFAULT_BUILDER_FEATURES', 'HTML_FORMATTERS', 'NO_PARSER_SPECIFIED_WARNING', 'ROOT_TAG_NAME', 'XML_FORMATTERS', '__bool__', '__call__', '__class__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_strings', '_check_markup_is_url', '_feed', '_find_all', '_find_one', '_formatter_for_name', '_is_xml', '_lastRecursiveChild', '_last_descendant', '_linkage_fixer', '_most_recent_element', '_namespaces', '_popToTag', '_should_pretty_print', 'append', 'attrs', 'builder', 'can_be_empty_element', 'childGenerator', 'children', 'clear', 'contains_replacement_characters', 'contents', 'currentTag', 'current_data', 'declared_html_encoding', 'decode', 'decode_contents', 'decompose', 'descendants', 'encode', 'encode_contents', 'endData', 'extend', 'extract', 'fetchNextSiblings', 'fetchParents', 'fetchPrevious', 'fetchPreviousSiblings', 'find', 'findAll', 'findAllNext', 'findAllPrevious', 'findChild', 'findChildren', 'findNext', 'findNextSibling', 'findNextSiblings', 'findParent', 'findParents', 'findPrevious', 'findPreviousSibling', 'findPreviousSiblings', 'find_all', 'find_all_next', 'find_all_previous', 'find_next', 'find_next_sibling', 'find_next_siblings', 'find_parent', 'find_parents', 'find_previous', 'find_previous_sibling', 'find_previous_siblings', 'format_string', 'get', 'getText', 'get_attribute_list', 'get_text', 'handle_data', 'handle_endtag', 'handle_starttag', 'has_attr', 'has_key', 'hidden', 'index', 'insert', 'insert_after', 'insert_before', 'isSelfClosing', 'is_empty_element', 'is_xml', 'known_xml', 'markup', 'name', 'namespace', 'new_string', 'new_tag', 'next', 'nextGenerator', 'nextSibling', 'nextSiblingGenerator', 'next_element', 'next_elements', 'next_sibling', 'next_siblings', 'object_was_parsed', 'original_encoding', 'parent', 'parentGenerator', 'parents', 'parse_only', 'parserClass', 'parser_class', 'popTag', 'prefix', 'preserve_whitespace_tag_stack', 'preserve_whitespace_tags', 'prettify', 'previous', 'previousGenerator', 'previousSibling', 'previousSiblingGenerator', 'previous_element', 'previous_elements', 'previous_sibling', 'previous_siblings', 'pushTag', 'recursiveChildGenerator', 'renderContents', 'replaceWith', 'replaceWithChildren', 'replace_with', 'replace_with_children', 'reset', 'select', 'select_one', 'setup', 'string', 'strings', 'stripped_strings', 'tagStack', 'text', 'unwrap', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "# There is a lot we can do with the soup object\n",
    "print(dir(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know a bit about html, we can quickly get back the first occurence of each of these HTML elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>BBQ &amp; Grilling Recipes - Allrecipes.com</title>\n",
      "<a id=\"top\"></a>\n",
      "<input id=\"searchText\" name=\"searchText\" ng-keypress=\"isEnterKey($event) &amp;&amp; performSearch()\" ng-model=\"search.keywords\" placeholder=\"Find a recipe\" type=\"text\"/>\n",
      "<option value=\"\">Select location</option>\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.a)\n",
    "print(soup.input)\n",
    "print(soup.option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "# As we can see we can get Tag elements from Beautiful Soup\n",
    "print(type(soup.div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HTML_FORMATTERS', 'XML_FORMATTERS', '__bool__', '__call__', '__class__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_strings', '_find_all', '_find_one', '_formatter_for_name', '_is_xml', '_lastRecursiveChild', '_last_descendant', '_should_pretty_print', 'append', 'attrs', 'can_be_empty_element', 'childGenerator', 'children', 'clear', 'contents', 'decode', 'decode_contents', 'decompose', 'descendants', 'encode', 'encode_contents', 'extend', 'extract', 'fetchNextSiblings', 'fetchParents', 'fetchPrevious', 'fetchPreviousSiblings', 'find', 'findAll', 'findAllNext', 'findAllPrevious', 'findChild', 'findChildren', 'findNext', 'findNextSibling', 'findNextSiblings', 'findParent', 'findParents', 'findPrevious', 'findPreviousSibling', 'findPreviousSiblings', 'find_all', 'find_all_next', 'find_all_previous', 'find_next', 'find_next_sibling', 'find_next_siblings', 'find_parent', 'find_parents', 'find_previous', 'find_previous_sibling', 'find_previous_siblings', 'format_string', 'get', 'getText', 'get_attribute_list', 'get_text', 'has_attr', 'has_key', 'hidden', 'index', 'insert', 'insert_after', 'insert_before', 'isSelfClosing', 'is_empty_element', 'known_xml', 'name', 'namespace', 'next', 'nextGenerator', 'nextSibling', 'nextSiblingGenerator', 'next_element', 'next_elements', 'next_sibling', 'next_siblings', 'parent', 'parentGenerator', 'parents', 'parserClass', 'parser_class', 'prefix', 'preserve_whitespace_tags', 'prettify', 'previous', 'previousGenerator', 'previousSibling', 'previousSiblingGenerator', 'previous_element', 'previous_elements', 'previous_sibling', 'previous_siblings', 'recursiveChildGenerator', 'renderContents', 'replaceWith', 'replaceWithChildren', 'replace_with', 'replace_with_children', 'select', 'select_one', 'setup', 'string', 'strings', 'stripped_strings', 'text', 'unwrap', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "# What can we do with these tag elements?\n",
    "print(dir(soup.div))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll come back to this list of methods later, but as you can see there are a lot of methods we can use to get information out of a Tag Element Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for elements using find() and find_all()\n",
    "\n",
    "As you might expect, ``find_all()`` Will find every occurence of an element on the page, where ``find()`` will return the first occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 328 on the page\n",
      "\n",
      "[<a id=\"top\"></a>, <a class=\"skip-to-content\" href=\"#main-content\">Skip to main content</a>, <a class=\"newThisMonth\" href=\"/new-this-month/\" rel=\"nofollow\">New&lt;&gt; this month</a>, <a aria-label=\"Pinterest\" class=\"pinterest\" data-header-link-tracking='{\"label\": \"Social &gt; Pinterest\"}' href=\"http://pinterest.com/allrecipes/\" target=\"_blank\" title=\"Pinterest\"><span class=\"svg-icon--social--pinterest svg-icon--social--pinterest-dims\"></span></a>, <a aria-label=\"Facebook\" class=\"facebook\" data-header-link-tracking='{\"label\": \"Social &gt; Facebook\"}' href=\"https://www.facebook.com/allrecipes\" target=\"_blank\" title=\"Facebook\"><span class=\"svg-icon--social--facebook svg-icon--social--facebook-dims\"></span></a>, <a aria-label=\"Instagram\" class=\"instagram\" data-header-link-tracking='{\"label\": \"Social &gt; Instagram\"}' href=\"http://instagram.com/allrecipes\" target=\"_blank\" title=\"Instagram\"><span class=\"svg-icon--social--instagram svg-icon--social--instagram-dims\"></span></a>, <a aria-label=\"Twitter\" class=\"twitter\" data-header-link-tracking='{\"label\": \"Social &gt; Twitter\"}' href=\"https://twitter.com/Allrecipes\" target=\"_blank\" title=\"Twitter\"><span class=\"svg-icon--social--twitter svg-icon--social--twitter-dims\"></span></a>, <a class=\"magazine-bar__link\" data-header-link-tracking='{\"label\": \"Magazine\"}' href=\"http://armagazine.com/upper-nav\" target=\"_blank\">Get the Allrecipes magazine</a>, <a aria-label=\"Allrecipes home page\" data-header-link-tracking='{\"label\": \"Brand Logo\"}' href=\"https://www.allrecipes.com\">\n",
      "<div class=\"ar-logo\" ng-click=\"setAnalyticsCookie('ARlogo')\">\n",
      "<img alt=\"Allrecipes\" height=\"36\" src=\"https://secureimages.allrecipes.com/ar-images/ARlogo.svg\" width=\"96\"/> </div>\n",
      "</a>, <a class=\"recipes-txt {active:topBrowseRecipePanel_showing}\" data-header-link-tracking='{\"label\": \"Browse\"}' href=\"\" id=\"navmenu_recipes\" popup-trigger=\"topBrowseRecipePanel\"><span>BROWSE</span><span class=\"icon--chevron-down\"></span></a>]\n"
     ]
    }
   ],
   "source": [
    "all_links = soup.find_all('a')\n",
    "print(f'There are {len(all_links)} on the page', end='\\n\\n')\n",
    "\n",
    "# The First 10 links on the page\n",
    "print(all_links[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p data-ellipsis=\"\" data-ng-cloak=\"\">Spice up your grilling rotation with some fiery Mexican-style BBQ. These recipes get their smoky, char-flavored kicks from the flames.</p>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering find_all with a function\n",
    "\n",
    "Based on the Beautiful Soup [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#a-function), it's possible to filter searches using a funtion that takes a single argument (an HTML Element). The function can be as complicated as you want, but should return True or False, whether to return that element or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_all_hidden(element):\n",
    "    return element.hidden\n",
    "\n",
    "p = soup.find('p')\n",
    "\n",
    "p.hidden\n",
    "# [item for item in dir(p) if 'p' in item]\n",
    "p.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks Like there aren't any hidden elements on the page\n",
    "soup.find_all(find_all_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example. if the parent element is a div\n",
    "from bs4.element import Tag\n",
    "\n",
    "def is_parent_a_div(element):\n",
    "    return isinstance(element.parent, Tag) and element.parent.name == 'div'\n",
    "\n",
    "elements_with_div_parents = soup.find_all(is_parent_a_div)\n",
    "len(elements_with_div_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find elements by CSS Class / and Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"toggle-similar__title\" itemprop=\"name\">\n",
      "                        Home\n",
      "                    </span>, <span class=\"toggle-similar__title\" itemprop=\"name\">\n",
      "                            Recipes\n",
      "                        </span>, <span class=\"toggle-similar__title\" itemprop=\"name\">\n",
      "                        BBQ &amp; Grilling\n",
      "                    </span>, <div class=\"title-section\">\n",
      "<h2 class=\"special-font\"></h2>\n",
      "<h1>\n",
      "<span class=\"title-section__text title\">BBQ &amp; Grilling Recipes</span>\n",
      "</h1>\n",
      "<span class=\"title-section__text subtitle\">The best BBQ chicken, pork and BBQ sauces. Hundreds of barbecue and grilling recipes, with tips and tricks from home grillers.</span>\n",
      "<div class=\"title-section__follow\" data-ng-cloak=\"\" data-ng-controller=\"ar_controllers_hub_stream\" data-ng-init=\"init('bq', 88, 'BBQ')\">\n",
      "<span class=\"hub-follow-blurb\">Follow to get the latest bbq &amp; grilling recipes, articles and more!</span>\n",
      "<a class=\"hub-follow\" ng-class=\"{'highlighted': isFollowing}\" ng-click=\"followStream()\">\n",
      "<span class=\"svg-icon--recipe-navbar--heart_off svg-icon--recipe-navbar--heart_off-dims\"></span>\n",
      "<span class=\"svg-icon--recipe-navbar--heart_on svg-icon--recipe-navbar--heart_on-dims\"></span>\n",
      "</a>\n",
      "</div>\n",
      "</div>, <span class=\"title-section__text title\">BBQ &amp; Grilling Recipes</span>]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "title_class = re.compile('title')\n",
    "\n",
    "elements_with_title_css = soup.find_all(class_=title_class)\n",
    "len(elements_with_title_css)\n",
    "\n",
    "print(elements_with_title_css[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Recipie Data\n",
    "\n",
    "We've seen how easy using the requests library makes it to send HTTP requests in Python. In conjuctino with Beautiful soup, we can quickly parse values out of HTML that might be returned from the request. In the next section we'll build a model for capturing data about each recepie.\n",
    "\n",
    "Go to your browser, type in the url that we used the request library to get for us earlier, and take a look at the page. There seems to be some structure to the way the recepeis are layed out.\n",
    "\n",
    "``Right click`` the page, and cick ``inspect``\n",
    "\n",
    "Taking a look at the HTML for the page will help us figure out what we need to search for when trying to pare the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Notes on Web Scraping\n",
    "\n",
    "Web Scraping in theory is pretty simple, you have some website that's written in HTML, once you have that html you utilise the built in structure of HTML to extract only the information that you need. In practice it's much more complicated. You might be able to build a script that can scrape one webiste, but what happens when the structure of the HTML changes? Suddenly, your script is obsolete and you might have to start from scratch. What about two differenct sites? Chances are the first script won't work for the second site. On top of that, a lot of websites / web applications are built with a minimal amout of HTML these days. Modern Web Frameworks utilise JavaScript to fully rendered pages; data is loaded asyncronously, which means a lot of information is usually not stored in the HTML sent to the browser; or more data is loaded when a user scrolls to a certain point on the page. If you're using the methods we showed today, it might not alwasy work. After all this you Also need to consider that some websites will actively try to prevent people from scraping their data, and it's not hard to figure out who's a bot and who's not.\n",
    "\n",
    "If you thought what we covered in todays class was interesting than I highly encourage you to keep at it. I would also add that it's important to be mindful, and not overload any one site with too many requests to their server at once. If you are scraping multiple pages of the same site, just take it slow.\n",
    "\n",
    "There are other cool tools for browser automation that we didn't have time to cover today (and won't have time to cover in this course), but if you're interested I'd highly recommend checking out a library called [selenium](https://selenium-python.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [an overveiw of HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview) Overall a ton of great info if you want to dig a bit deeper on how the web works\n",
    "* [list of HTTP Status Codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)\n",
    "* [requests library](https://realpython.com/python-requests/)\n",
    "* [Beautiful Soup tutorial](https://www.youtube.com/watch?v=87Gx3U0BDlo)\n",
    "* [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [Web Scraping Introduction, Best Practices, and Caveats](https://medium.com/velotio-perspectives/web-scraping-introduction-best-practices-caveats-9cbf4acc8d0f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
